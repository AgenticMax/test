# AgentRL å¼€å‘è€…æŒ‡å—

## ğŸ¯ å¼€å‘ç¯å¢ƒè®¾ç½®

### 1. å…‹éš†å’Œå®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/agentrl/agentrl.git
cd agentrl

# åˆ›å»ºå¼€å‘ç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements.txt
pip install -e .  # å¯ç¼–è¾‘å®‰è£…

# å®‰è£…å¼€å‘å·¥å…·
pip install pytest black flake8 mypy pre-commit
```

### 2. å¼€å‘å·¥å…·é…ç½®

#### Pre-commit é’©å­

```bash
# å®‰è£… pre-commit é’©å­
pre-commit install

# æ‰‹åŠ¨è¿è¡Œæ£€æŸ¥
pre-commit run --all-files
```

#### VSCode é…ç½®

`.vscode/settings.json`:
```json
{
    "python.defaultInterpreterPath": "./venv/bin/python",
    "python.linting.enabled": true,
    "python.linting.flake8Enabled": true,
    "python.formatting.provider": "black",
    "python.formatting.blackArgs": ["--line-length=88"],
    "python.testing.pytestEnabled": true,
    "python.testing.unittestEnabled": false,
    "python.testing.pytestArgs": ["tests/"]
}
```

## ğŸ—ï¸ æ¡†æ¶æ‰©å±•æŒ‡å—

### 1. æ·»åŠ æ–°ç®—æ³•

#### æ­¥éª¤1ï¼šåˆ›å»ºç®—æ³•ç±»

```python
# agentrl/algorithms/new_algorithm.py
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Union
from ..core.agent import BaseAgent
from ..core.network import ActorCriticNetwork
from ..core.memory import ReplayBuffer

class NewAlgorithmAgent(BaseAgent):
    """æ–°ç®—æ³•æ™ºèƒ½ä½“å®ç°"""
    
    def __init__(self, 
                 observation_space: Any,
                 action_space: Any,
                 config: Dict[str, Any]):
        super().__init__(observation_space, action_space, config)
        
        # ç®—æ³•ç‰¹å®šé…ç½®
        self.lr = config.get("lr", 3e-4)
        self.gamma = config.get("gamma", 0.99)
        # ... å…¶ä»–å‚æ•°
        
        # åˆ›å»ºç½‘ç»œ
        self.network = self._build_network()
        self.optimizer = torch.optim.Adam(
            self.network.parameters(), lr=self.lr
        )
        
        # åˆ›å»ºå†…å­˜
        self.memory = ReplayBuffer(
            capacity=config.get("memory_size", 100000),
            obs_shape=self.obs_shape,
            action_shape=(self.action_dim,),
            device=self.device
        )
    
    def _build_network(self):
        """æ„å»ºç¥ç»ç½‘ç»œ"""
        hidden_dims = self.config.get("hidden_dims", [64, 64])
        return ActorCriticNetwork(
            input_dim=np.prod(self.obs_shape),
            action_dim=self.action_dim,
            hidden_dims=hidden_dims,
            continuous_actions=self.continuous_actions
        )
    
    def act(self, observation: np.ndarray, training: bool = True) -> Union[int, np.ndarray]:
        """é€‰æ‹©åŠ¨ä½œ"""
        obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            if self.continuous_actions:
                # è¿ç»­åŠ¨ä½œç©ºé—´
                action_logits, _ = self.network(obs_tensor)
                action = torch.tanh(action_logits)
                
                if training:
                    # æ·»åŠ æ¢ç´¢å™ªå£°
                    noise = torch.randn_like(action) * 0.1
                    action = torch.clamp(action + noise, -1, 1)
                
                return action.cpu().numpy()[0]
            else:
                # ç¦»æ•£åŠ¨ä½œç©ºé—´
                action_logits, _ = self.network(obs_tensor)
                
                if training:
                    # epsilon-greedy æ¢ç´¢
                    if np.random.random() < self.epsilon:
                        return np.random.randint(self.action_dim)
                
                return torch.argmax(action_logits, dim=1).item()
    
    def learn(self, batch: Dict[str, torch.Tensor] = None) -> Dict[str, float]:
        """å­¦ä¹ æ›´æ–°"""
        if batch is None:
            if len(self.memory) < self.config.get("batch_size", 32):
                return {}
            batch = self.memory.sample(self.config.get("batch_size", 32))
        
        # å®ç°ç®—æ³•ç‰¹å®šçš„å­¦ä¹ é€»è¾‘
        obs = batch["observations"]
        actions = batch["actions"] 
        rewards = batch["rewards"]
        next_obs = batch["next_observations"]
        dones = batch["dones"]
        
        # è®¡ç®—æŸå¤±
        loss = self._compute_loss(obs, actions, rewards, next_obs, dones)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return {"loss": loss.item()}
    
    def _compute_loss(self, obs, actions, rewards, next_obs, dones):
        """è®¡ç®—æŸå¤±å‡½æ•°"""
        # å®ç°å…·ä½“çš„æŸå¤±è®¡ç®—
        pass
    
    def save(self, filepath: str) -> None:
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'network_state_dict': self.network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config
        }, filepath)
    
    def load(self, filepath: str) -> None:
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.network.load_state_dict(checkpoint['network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

#### æ­¥éª¤2ï¼šæ³¨å†Œç®—æ³•

åœ¨ `agentrl/core/agent.py` ä¸­æ³¨å†Œæ–°ç®—æ³•ï¼š

```python
def _create_agent(self) -> BaseAgent:
    """æ ¹æ®ç®—æ³•ç±»å‹åˆ›å»ºæ™ºèƒ½ä½“"""
    if self.algorithm == "PPO":
        from ..algorithms.ppo import PPOAgent
        return PPOAgent(self.observation_space, self.action_space, self.config)
    # ... å…¶ä»–ç®—æ³•
    elif self.algorithm == "NEW_ALGORITHM":
        from ..algorithms.new_algorithm import NewAlgorithmAgent
        return NewAlgorithmAgent(self.observation_space, self.action_space, self.config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")
```

#### æ­¥éª¤3ï¼šæ›´æ–°é»˜è®¤é…ç½®

åœ¨ `agentrl/utils/config.py` ä¸­æ·»åŠ é»˜è®¤é…ç½®ï¼š

```python
def create_default_config() -> Config:
    default_config = {
        # ... ç°æœ‰é…ç½®
        
        # æ–°ç®—æ³•é…ç½®
        "new_algorithm": {
            "lr": 3e-4,
            "gamma": 0.99,
            "batch_size": 32,
            "memory_size": 100000,
            "hidden_dims": [64, 64],
            # ... å…¶ä»–å‚æ•°
        }
    }
    return Config(default_config)
```

#### æ­¥éª¤4ï¼šåˆ›å»ºä¸“ç”¨è®­ç»ƒå™¨ï¼ˆå¯é€‰ï¼‰

```python
# agentrl/training/new_algorithm_trainer.py
from .trainer import BaseTrainer

class NewAlgorithmTrainer(BaseTrainer):
    """æ–°ç®—æ³•ä¸“ç”¨è®­ç»ƒå™¨"""
    
    def __init__(self, agent, env, config=None):
        super().__init__(agent, env, config)
        # ç®—æ³•ç‰¹å®šçš„è®­ç»ƒé…ç½®
    
    def train_step(self, episode: int) -> Dict[str, Any]:
        """è®­ç»ƒæ­¥éª¤å®ç°"""
        # å®ç°ç®—æ³•ç‰¹å®šçš„è®­ç»ƒé€»è¾‘
        pass
```

### 2. æ·»åŠ æ–°ç½‘ç»œæ¶æ„

#### åˆ›å»ºç½‘ç»œç±»

```python
# agentrl/core/network.py ä¸­æ·»åŠ 
class CustomNetwork(nn.Module):
    """è‡ªå®šä¹‰ç½‘ç»œæ¶æ„"""
    
    def __init__(self, input_dim, output_dim, **kwargs):
        super().__init__()
        
        # å®ç°ç½‘ç»œç»“æ„
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        
        self.output_layer = nn.Linear(128, output_dim)
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        features = self.feature_extractor(x)
        return self.output_layer(features)
```

### 3. æ·»åŠ æ–°ç¯å¢ƒæ”¯æŒ

#### åˆ›å»ºç¯å¢ƒåŒ…è£…å™¨

```python
# agentrl/core/environment.py ä¸­æ·»åŠ 
class CustomEnvironmentWrapper:
    """è‡ªå®šä¹‰ç¯å¢ƒåŒ…è£…å™¨"""
    
    def __init__(self, env, config=None):
        self.env = env
        self.config = config or {}
        
        # å®ç°ç‰¹å®šçš„ç¯å¢ƒå¢å¼º
        self.observation_space = self._modify_observation_space()
        self.action_space = self._modify_action_space()
    
    def reset(self):
        obs = self.env.reset()
        return self._preprocess_observation(obs)
    
    def step(self, action):
        action = self._preprocess_action(action)
        obs, reward, done, info = self.env.step(action)
        
        obs = self._preprocess_observation(obs)
        reward = self._preprocess_reward(reward)
        
        return obs, reward, done, info
    
    def _preprocess_observation(self, obs):
        """é¢„å¤„ç†è§‚æµ‹"""
        # å®ç°è§‚æµ‹é¢„å¤„ç†é€»è¾‘
        return obs
    
    def _preprocess_action(self, action):
        """é¢„å¤„ç†åŠ¨ä½œ"""
        # å®ç°åŠ¨ä½œé¢„å¤„ç†é€»è¾‘
        return action
    
    def _preprocess_reward(self, reward):
        """é¢„å¤„ç†å¥–åŠ±"""
        # å®ç°å¥–åŠ±é¢„å¤„ç†é€»è¾‘
        return reward
```

## ğŸ§ª æµ‹è¯•æŒ‡å—

### 1. æµ‹è¯•ç»“æ„

```
tests/
â”œâ”€â”€ unit/                   # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_agent.py
â”‚   â”œâ”€â”€ test_network.py
â”‚   â”œâ”€â”€ test_memory.py
â”‚   â””â”€â”€ test_environment.py
â”œâ”€â”€ integration/            # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_training.py
â”‚   â””â”€â”€ test_inference.py
â”œâ”€â”€ algorithms/             # ç®—æ³•æµ‹è¯•
â”‚   â”œâ”€â”€ test_ppo.py
â”‚   â”œâ”€â”€ test_dqn.py
â”‚   â””â”€â”€ test_sac.py
â””â”€â”€ conftest.py            # æµ‹è¯•é…ç½®
```

### 2. ç¼–å†™æµ‹è¯•

#### å•å…ƒæµ‹è¯•ç¤ºä¾‹

```python
# tests/unit/test_agent.py
import pytest
import numpy as np
import gymnasium as gym
from agentrl import Agent

class TestAgent:
    """æ™ºèƒ½ä½“æµ‹è¯•ç±»"""
    
    @pytest.fixture
    def cartpole_env(self):
        """CartPoleç¯å¢ƒfixture"""
        return gym.make("CartPole-v1")
    
    @pytest.fixture
    def ppo_agent(self, cartpole_env):
        """PPOæ™ºèƒ½ä½“fixture"""
        return Agent("PPO", cartpole_env.observation_space, cartpole_env.action_space)
    
    def test_agent_creation(self, ppo_agent):
        """æµ‹è¯•æ™ºèƒ½ä½“åˆ›å»º"""
        assert ppo_agent is not None
        assert hasattr(ppo_agent, 'act')
        assert hasattr(ppo_agent, 'learn')
    
    def test_action_selection(self, ppo_agent, cartpole_env):
        """æµ‹è¯•åŠ¨ä½œé€‰æ‹©"""
        obs, _ = cartpole_env.reset()
        action = ppo_agent.act(obs)
        
        assert isinstance(action, (int, np.integer))
        assert 0 <= action < cartpole_env.action_space.n
    
    def test_save_load(self, ppo_agent, tmp_path):
        """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
        filepath = tmp_path / "test_model.pt"
        
        # ä¿å­˜æ¨¡å‹
        ppo_agent.save(str(filepath))
        assert filepath.exists()
        
        # åŠ è½½æ¨¡å‹
        ppo_agent.load(str(filepath))
```

#### é›†æˆæµ‹è¯•ç¤ºä¾‹

```python
# tests/integration/test_training.py
import pytest
from agentrl import Agent, Environment, PPOTrainer

class TestTraining:
    """è®­ç»ƒé›†æˆæµ‹è¯•"""
    
    def test_ppo_training(self):
        """æµ‹è¯•PPOè®­ç»ƒæµç¨‹"""
        env = Environment("CartPole-v1")
        agent = Agent("PPO", env.observation_space, env.action_space)
        trainer = PPOTrainer(agent, env)
        
        # çŸ­æœŸè®­ç»ƒæµ‹è¯•
        history = trainer.train(max_episodes=10)
        
        assert len(history["episode_rewards"]) == 10
        assert all(isinstance(r, (int, float)) for r in history["episode_rewards"])
    
    def test_evaluation(self):
        """æµ‹è¯•è¯„ä¼°åŠŸèƒ½"""
        env = Environment("CartPole-v1")
        agent = Agent("PPO", env.observation_space, env.action_space)
        trainer = PPOTrainer(agent, env)
        
        # è¯„ä¼°æœªè®­ç»ƒçš„æ™ºèƒ½ä½“
        results = trainer.evaluate(episodes=5)
        
        assert "mean_reward" in results
        assert "std_reward" in results
        assert len(results["episode_rewards"]) == 5
```

### 3. è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶
pytest tests/unit/test_agent.py

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–¹æ³•
pytest tests/unit/test_agent.py::TestAgent::test_agent_creation

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=agentrl --cov-report=html

# å¹¶è¡Œè¿è¡Œæµ‹è¯•
pytest -n auto  # éœ€è¦å®‰è£… pytest-xdist
```

## ğŸ“ ä»£ç è§„èŒƒ

### 1. ä»£ç é£æ ¼

#### Python ç¼–ç è§„èŒƒ

- éµå¾ª PEP 8 æ ‡å‡†
- ä½¿ç”¨ Black æ ¼å¼åŒ–ä»£ç 
- è¡Œé•¿åº¦é™åˆ¶ä¸º 88 å­—ç¬¦
- ä½¿ç”¨ç±»å‹æç¤º

#### ç¤ºä¾‹ä»£ç 

```python
from typing import Dict, List, Optional, Union
import numpy as np
import torch


class ExampleClass:
    """ç¤ºä¾‹ç±»çš„æ–‡æ¡£å­—ç¬¦ä¸²
    
    è¯¦ç»†æè¿°ç±»çš„åŠŸèƒ½å’Œç”¨é€”ã€‚
    
    Args:
        param1: å‚æ•°1çš„æè¿°
        param2: å‚æ•°2çš„æè¿°
    """
    
    def __init__(self, param1: int, param2: Optional[str] = None) -> None:
        self.param1 = param1
        self.param2 = param2
    
    def example_method(
        self, 
        data: np.ndarray, 
        config: Dict[str, Union[int, float]]
    ) -> List[float]:
        """ç¤ºä¾‹æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²
        
        Args:
            data: è¾“å…¥æ•°æ®
            config: é…ç½®å‚æ•°
            
        Returns:
            å¤„ç†åçš„ç»“æœåˆ—è¡¨
            
        Raises:
            ValueError: å½“è¾“å…¥æ•°æ®æ ¼å¼é”™è¯¯æ—¶
        """
        if data.ndim != 2:
            raise ValueError("æ•°æ®å¿…é¡»æ˜¯äºŒç»´æ•°ç»„")
        
        # æ–¹æ³•å®ç°
        result = []
        for row in data:
            processed = self._process_row(row, config)
            result.append(processed)
        
        return result
    
    def _process_row(self, row: np.ndarray, config: Dict) -> float:
        """ç§æœ‰æ–¹æ³•ç¤ºä¾‹"""
        # å®ç°ç»†èŠ‚
        return float(np.mean(row))
```

### 2. æ–‡æ¡£å­—ç¬¦ä¸²è§„èŒƒ

ä½¿ç”¨ Google é£æ ¼çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼š

```python
def train_agent(
    agent: BaseAgent,
    env: Environment,
    episodes: int,
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, List[float]]:
    """è®­ç»ƒæ™ºèƒ½ä½“
    
    åœ¨æŒ‡å®šç¯å¢ƒä¸­è®­ç»ƒæ™ºèƒ½ä½“æŒ‡å®šçš„å›åˆæ•°ã€‚
    
    Args:
        agent: è¦è®­ç»ƒçš„æ™ºèƒ½ä½“
        env: è®­ç»ƒç¯å¢ƒ
        episodes: è®­ç»ƒå›åˆæ•°
        config: å¯é€‰çš„è®­ç»ƒé…ç½®
        
    Returns:
        åŒ…å«è®­ç»ƒå†å²çš„å­—å…¸ï¼Œé”®åŒ…æ‹¬ï¼š
        - "episode_rewards": æ¯å›åˆå¥–åŠ±åˆ—è¡¨
        - "episode_lengths": æ¯å›åˆé•¿åº¦åˆ—è¡¨
        
    Raises:
        ValueError: å½“episodeså°äºç­‰äº0æ—¶
        RuntimeError: å½“è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶
        
    Examples:
        >>> agent = Agent("PPO", env.observation_space, env.action_space)
        >>> env = Environment("CartPole-v1")
        >>> history = train_agent(agent, env, episodes=1000)
        >>> print(f"å¹³å‡å¥–åŠ±: {np.mean(history['episode_rewards'])}")
    """
    # å®ç°
    pass
```

### 3. é”™è¯¯å¤„ç†

```python
def validate_config(config: Dict[str, Any]) -> None:
    """éªŒè¯é…ç½®å‚æ•°"""
    required_keys = ["lr", "gamma", "hidden_dims"]
    
    for key in required_keys:
        if key not in config:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„é…ç½®å‚æ•°: {key}")
    
    if not 0 < config["lr"] <= 1:
        raise ValueError(f"å­¦ä¹ ç‡å¿…é¡»åœ¨(0, 1]èŒƒå›´å†…ï¼Œå½“å‰å€¼: {config['lr']}")
    
    if not 0 <= config["gamma"] <= 1:
        raise ValueError(f"æŠ˜æ‰£å› å­å¿…é¡»åœ¨[0, 1]èŒƒå›´å†…ï¼Œå½“å‰å€¼: {config['gamma']}")

def safe_train(agent, env, episodes):
    """å®‰å…¨è®­ç»ƒåŒ…è£…å™¨"""
    try:
        return train_agent(agent, env, episodes)
    except ValueError as e:
        logger.error(f"é…ç½®é”™è¯¯: {e}")
        raise
    except RuntimeError as e:
        logger.error(f"è¿è¡Œæ—¶é”™è¯¯: {e}")
        # æ¸…ç†èµ„æº
        cleanup_resources()
        raise
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
        raise RuntimeError(f"è®­ç»ƒå¤±è´¥: {e}") from e
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. æ€§èƒ½åˆ†æ

```python
# ä½¿ç”¨ cProfile è¿›è¡Œæ€§èƒ½åˆ†æ
import cProfile
import pstats

def profile_training():
    """åˆ†æè®­ç»ƒæ€§èƒ½"""
    profiler = cProfile.Profile()
    profiler.enable()
    
    # è¿è¡Œè®­ç»ƒä»£ç 
    train_agent(agent, env, episodes=100)
    
    profiler.disable()
    
    # åˆ†æç»“æœ
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # æ˜¾ç¤ºå‰20ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
```

### 2. å†…å­˜ä¼˜åŒ–

```python
import tracemalloc
import gc

def monitor_memory():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    tracemalloc.start()
    
    # è¿è¡Œä»£ç 
    train_agent(agent, env, episodes=100)
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    current, peak = tracemalloc.get_traced_memory()
    print(f"å½“å‰å†…å­˜ä½¿ç”¨: {current / 1024 / 1024:.2f} MB")
    print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {peak / 1024 / 1024:.2f} MB")
    
    tracemalloc.stop()
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
```

### 3. GPU ä¼˜åŒ–

```python
def optimize_gpu_usage():
    """ä¼˜åŒ–GPUä½¿ç”¨"""
    if torch.cuda.is_available():
        # å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        scaler = torch.cuda.amp.GradScaler()
        
        # ä½¿ç”¨autocast
        with torch.cuda.amp.autocast():
            loss = compute_loss(batch)
        
        # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
```

## ğŸ“Š åŸºå‡†æµ‹è¯•

### 1. æ€§èƒ½åŸºå‡†

```python
# benchmarks/performance_benchmark.py
import time
import numpy as np
from agentrl import Agent, Environment, InferenceEngine

def benchmark_inference_speed():
    """åŸºå‡†æµ‹è¯•æ¨ç†é€Ÿåº¦"""
    env = Environment("CartPole-v1")
    agent = Agent("PPO", env.observation_space, env.action_space)
    engine = InferenceEngine(agent, batch_size=32)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    observations = np.random.randn(1000, 4)
    
    # é¢„çƒ­
    for _ in range(10):
        engine.predict(observations[:32])
    
    # åŸºå‡†æµ‹è¯•
    start_time = time.time()
    for i in range(0, len(observations), 32):
        batch = observations[i:i+32]
        actions = engine.predict(batch)
    end_time = time.time()
    
    total_time = end_time - start_time
    fps = len(observations) / total_time
    
    print(f"æ¨ç†é€Ÿåº¦: {fps:.2f} FPS")
    print(f"å¹³å‡å»¶è¿Ÿ: {total_time / len(observations) * 1000:.3f} ms")

def benchmark_training_speed():
    """åŸºå‡†æµ‹è¯•è®­ç»ƒé€Ÿåº¦"""
    env = Environment("CartPole-v1")
    agent = Agent("PPO", env.observation_space, env.action_space)
    trainer = PPOTrainer(agent, env)
    
    start_time = time.time()
    trainer.train(max_episodes=100)
    end_time = time.time()
    
    print(f"è®­ç»ƒ100å›åˆè€—æ—¶: {end_time - start_time:.2f} ç§’")
```

## ğŸ”„ æŒç»­é›†æˆ

### 1. GitHub Actions é…ç½®

`.github/workflows/ci.yml`:
```yaml
name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, "3.10", "3.11"]

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .
    
    - name: Lint with flake8
      run: |
        pip install flake8
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88
    
    - name: Test with pytest
      run: |
        pip install pytest pytest-cov
        pytest --cov=agentrl --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### 2. ä»£ç è´¨é‡æ£€æŸ¥

`.pre-commit-config.yaml`:
```yaml
repos:
  - repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
      - id: black
        language_version: python3
        args: [--line-length=88]

  - repo: https://github.com/pycqa/flake8
    rev: 4.0.1
    hooks:
      - id: flake8
        args: [--max-line-length=88, --ignore=E203,W503]

  - repo: https://github.com/pycqa/isort
    rev: 5.10.1
    hooks:
      - id: isort
        args: [--profile=black]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v0.942
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
```

## ğŸ“‹ è´¡çŒ®æµç¨‹

### 1. åˆ†æ”¯ç®¡ç†

- `main`: ç¨³å®šç‰ˆæœ¬åˆ†æ”¯
- `develop`: å¼€å‘åˆ†æ”¯
- `feature/*`: åŠŸèƒ½å¼€å‘åˆ†æ”¯
- `bugfix/*`: é”™è¯¯ä¿®å¤åˆ†æ”¯
- `release/*`: å‘å¸ƒå‡†å¤‡åˆ†æ”¯

### 2. æäº¤è§„èŒƒ

ä½¿ç”¨çº¦å®šå¼æäº¤ï¼ˆConventional Commitsï¼‰ï¼š

```
<type>[optional scope]: <description>

[optional body]

[optional footer(s)]
```

ç¤ºä¾‹ï¼š
```
feat(algorithm): add SAC algorithm implementation

- Implement Soft Actor-Critic algorithm
- Add automatic entropy tuning
- Support continuous action spaces

Closes #123
```

ç±»å‹è¯´æ˜ï¼š
- `feat`: æ–°åŠŸèƒ½
- `fix`: é”™è¯¯ä¿®å¤
- `docs`: æ–‡æ¡£æ›´æ–°
- `style`: ä»£ç æ ¼å¼åŒ–
- `refactor`: é‡æ„
- `test`: æµ‹è¯•ç›¸å…³
- `chore`: æ„å»ºè¿‡ç¨‹æˆ–è¾…åŠ©å·¥å…·å˜åŠ¨

### 3. Pull Request æµç¨‹

1. Fork ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. ç¼–å†™ä»£ç å’Œæµ‹è¯•
4. ç¡®ä¿æµ‹è¯•é€šè¿‡
5. æäº¤ Pull Request
6. ä»£ç å®¡æŸ¥
7. åˆå¹¶åˆ°ä¸»åˆ†æ”¯

## ğŸ¯ æœ€ä½³å®è·µ

1. **ç¼–å†™æµ‹è¯•**: æ‰€æœ‰æ–°åŠŸèƒ½éƒ½åº”è¯¥æœ‰å¯¹åº”çš„æµ‹è¯•
2. **æ–‡æ¡£å®Œæ•´**: æä¾›æ¸…æ™°çš„æ–‡æ¡£å’Œç¤ºä¾‹
3. **æ€§èƒ½è€ƒè™‘**: æ³¨æ„ä»£ç æ€§èƒ½ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—
4. **å‘åå…¼å®¹**: å°½é‡ä¿æŒAPIçš„å‘åå…¼å®¹æ€§
5. **æ¨¡å—åŒ–è®¾è®¡**: ä¿æŒä»£ç çš„æ¨¡å—åŒ–å’Œå¯å¤ç”¨æ€§

é€šè¿‡éµå¾ªè¿™äº›å¼€å‘æŒ‡å—ï¼Œæ‚¨å¯ä»¥æœ‰æ•ˆåœ°ä¸º AgentRL æ¡†æ¶è´¡çŒ®ä»£ç ï¼Œæ‰©å±•åŠŸèƒ½ï¼Œå¹¶ç»´æŠ¤é«˜è´¨é‡çš„ä»£ç åº“ã€‚