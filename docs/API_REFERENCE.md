# AgentRL API å‚è€ƒæ–‡æ¡£

## ğŸ“š æ ¸å¿ƒAPIæ¦‚è§ˆ

AgentRLæ¡†æ¶æä¾›äº†ç®€æ´è€Œå¼ºå¤§çš„APIæ¥å£ï¼Œæ”¯æŒå¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒå’Œæ¨ç†ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†æ‰€æœ‰ä¸»è¦ç±»ã€æ–¹æ³•å’Œé…ç½®é€‰é¡¹ã€‚

## ğŸ¯ å¿«é€Ÿå¼€å§‹

```python
from agentrl import Agent, Environment, PPOTrainer
from agentrl.utils.config import create_default_config

# åˆ›å»ºé…ç½®
config = create_default_config()

# åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
env = Environment("CartPole-v1")
agent = Agent("PPO", env.observation_space, env.action_space)

# å¼€å§‹è®­ç»ƒ
trainer = PPOTrainer(agent, env, config)
trainer.train(episodes=1000)
```

## ğŸ—ï¸ æ ¸å¿ƒç±» API

### 1. Agent ç±»

æ™ºèƒ½ä½“å·¥å‚ç±»ï¼Œç”¨äºåˆ›å»ºå’Œç®¡ç†ä¸åŒç®—æ³•çš„æ™ºèƒ½ä½“ã€‚

#### åˆå§‹åŒ–

```python
Agent(algorithm, observation_space, action_space, config=None)
```

**å‚æ•°ï¼š**
- `algorithm` (str): ç®—æ³•åç§°ï¼Œæ”¯æŒ "PPO", "DQN", "SAC", "A3C"
- `observation_space`: è§‚æµ‹ç©ºé—´ï¼ŒGymnasiumæ ¼å¼
- `action_space`: åŠ¨ä½œç©ºé—´ï¼ŒGymnasiumæ ¼å¼  
- `config` (dict, optional): æ™ºèƒ½ä½“é…ç½®å‚æ•°

**ç¤ºä¾‹ï¼š**
```python
# åˆ›å»ºPPOæ™ºèƒ½ä½“
agent = Agent("PPO", env.observation_space, env.action_space, {
    "lr": 3e-4,
    "gamma": 0.99,
    "hidden_dims": [64, 64]
})

# åˆ›å»ºDQNæ™ºèƒ½ä½“
agent = Agent("DQN", env.observation_space, env.action_space, {
    "lr": 1e-4,
    "epsilon": 0.1,
    "target_update_freq": 1000
})
```

#### æ–¹æ³•

##### `act(observation, training=True)`

é€‰æ‹©åŠ¨ä½œã€‚

**å‚æ•°ï¼š**
- `observation` (np.ndarray): å½“å‰è§‚æµ‹
- `training` (bool): æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼

**è¿”å›ï¼š**
- `action`: é€‰æ‹©çš„åŠ¨ä½œï¼ˆintæˆ–np.ndarrayï¼‰

**ç¤ºä¾‹ï¼š**
```python
obs = env.reset()
action = agent.act(obs, training=True)  # è®­ç»ƒæ—¶
action = agent.act(obs, training=False) # è¯„ä¼°æ—¶
```

##### `learn(batch)`

å­¦ä¹ æ›´æ–°æ™ºèƒ½ä½“ã€‚

**å‚æ•°ï¼š**
- `batch` (dict): è®­ç»ƒæ‰¹æ¬¡æ•°æ®

**è¿”å›ï¼š**
- `dict`: è®­ç»ƒæŸå¤±ä¿¡æ¯

##### `save(filepath)` / `load(filepath)`

ä¿å­˜/åŠ è½½æ¨¡å‹ã€‚

**å‚æ•°ï¼š**
- `filepath` (str): æ–‡ä»¶è·¯å¾„

**ç¤ºä¾‹ï¼š**
```python
agent.save("models/ppo_cartpole.pt")
agent.load("models/ppo_cartpole.pt")
```

### 2. Environment ç±»

ç¯å¢ƒå°è£…ç±»ï¼Œæä¾›Gymnasiumç¯å¢ƒçš„å¢å¼ºåŠŸèƒ½ã€‚

#### åˆå§‹åŒ–

```python
Environment(env_id, config=None, **kwargs)
```

**å‚æ•°ï¼š**
- `env_id` (str): ç¯å¢ƒIDï¼Œå¦‚ "CartPole-v1"
- `config` (dict, optional): ç¯å¢ƒé…ç½®
- `**kwargs`: ä¼ é€’ç»™gymnasium.makeçš„é¢å¤–å‚æ•°

**é…ç½®é€‰é¡¹ï¼š**
```python
env_config = {
    "frame_stack": 4,           # å¸§å †å æ•°é‡
    "frame_skip": 1,            # å¸§è·³è·ƒæ•°é‡
    "normalize_obs": True,      # è§‚æµ‹å½’ä¸€åŒ–
    "normalize_reward": True,   # å¥–åŠ±å½’ä¸€åŒ–
    "clip_reward": 1.0,         # å¥–åŠ±è£å‰ª
    "max_episode_steps": 1000   # æœ€å¤§æ­¥æ•°
}
```

#### æ–¹æ³•

##### `reset()`

é‡ç½®ç¯å¢ƒã€‚

**è¿”å›ï¼š**
- `observation`: åˆå§‹è§‚æµ‹

##### `step(action)`

æ‰§è¡ŒåŠ¨ä½œã€‚

**å‚æ•°ï¼š**
- `action`: è¦æ‰§è¡Œçš„åŠ¨ä½œ

**è¿”å›ï¼š**
- `observation`: ä¸‹ä¸€ä¸ªè§‚æµ‹
- `reward`: å¥–åŠ±
- `done`: æ˜¯å¦ç»“æŸ
- `info`: é¢å¤–ä¿¡æ¯

**ç¤ºä¾‹ï¼š**
```python
env = Environment("CartPole-v1", {
    "normalize_obs": True,
    "max_episode_steps": 500
})

obs = env.reset()
action = agent.act(obs)
next_obs, reward, done, info = env.step(action)
```

### 3. è®­ç»ƒå™¨ç±»

#### BaseTrainer

è®­ç»ƒå™¨åŸºç±»ï¼Œå®šä¹‰äº†é€šç”¨çš„è®­ç»ƒæµç¨‹ã€‚

```python
BaseTrainer(agent, env, config=None)
```

**é€šç”¨é…ç½®ï¼š**
```python
training_config = {
    "max_episodes": 1000,
    "max_steps_per_episode": 1000,
    "eval_freq": 100,
    "save_freq": 100,
    "log_freq": 10,
    "early_stopping_patience": 100,
    "target_reward": None
}
```

##### `train(max_episodes=None, early_stopping_patience=100, target_reward=None)`

å¼€å§‹è®­ç»ƒè¿‡ç¨‹ã€‚

**å‚æ•°ï¼š**
- `max_episodes` (int, optional): æœ€å¤§è®­ç»ƒå›åˆæ•°
- `early_stopping_patience` (int): æ—©åœè€å¿ƒå€¼
- `target_reward` (float, optional): ç›®æ ‡å¥–åŠ±é˜ˆå€¼

**è¿”å›ï¼š**
- `dict`: è®­ç»ƒå†å²æ•°æ®

##### `evaluate(episodes=10, render=False)`

è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½ã€‚

**å‚æ•°ï¼š**
- `episodes` (int): è¯„ä¼°å›åˆæ•°
- `render` (bool): æ˜¯å¦æ¸²æŸ“

**è¿”å›ï¼š**
- `dict`: è¯„ä¼°ç»“æœ

#### PPOTrainer

PPOç®—æ³•ä¸“ç”¨è®­ç»ƒå™¨ã€‚

```python
PPOTrainer(agent, env, config=None)
```

**PPOç‰¹å®šé…ç½®ï¼š**
```python
ppo_config = {
    "update_interval": 2048,    # æ›´æ–°é—´éš”
    "ppo_epochs": 4,           # PPOè®­ç»ƒè½®æ•°
    "clip_ratio": 0.2,         # è£å‰ªæ¯”ä¾‹
    "value_loss_coef": 0.5,    # ä»·å€¼æŸå¤±ç³»æ•°
    "entropy_coef": 0.01       # ç†µç³»æ•°
}
```

#### DQNTrainer

DQNç®—æ³•ä¸“ç”¨è®­ç»ƒå™¨ã€‚

```python
DQNTrainer(agent, env, config=None)
```

**DQNç‰¹å®šé…ç½®ï¼š**
```python
dqn_config = {
    "memory_size": 100000,      # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
    "batch_size": 32,           # æ‰¹é‡å¤§å°
    "target_update_freq": 1000, # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
    "epsilon_decay": 0.995,     # epsilonè¡°å‡
    "min_epsilon": 0.01         # æœ€å°epsilon
}
```

### 4. InferenceEngine ç±»

é«˜æ•ˆæ¨ç†å¼•æ“ï¼Œç”¨äºæ¨¡å‹éƒ¨ç½²å’Œæ‰¹é‡æ¨ç†ã€‚

#### åˆå§‹åŒ–

```python
InferenceEngine(agent, device=None, batch_size=1, optimize=True)
```

**å‚æ•°ï¼š**
- `agent`: è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
- `device` (str, optional): æ¨ç†è®¾å¤‡ ("cpu" æˆ– "cuda")
- `batch_size` (int): æ‰¹é‡æ¨ç†å¤§å°
- `optimize` (bool): æ˜¯å¦å¯ç”¨æ¨¡å‹ä¼˜åŒ–

#### æ–¹æ³•

##### `predict(observations, deterministic=True)`

æ‰¹é‡é¢„æµ‹åŠ¨ä½œã€‚

**å‚æ•°ï¼š**
- `observations` (np.ndarray): è§‚æµ‹æ•°ç»„
- `deterministic` (bool): æ˜¯å¦ç¡®å®šæ€§é¢„æµ‹

**è¿”å›ï¼š**
- `actions`: é¢„æµ‹çš„åŠ¨ä½œ

##### `benchmark(num_samples=1000)`

æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚

**å‚æ•°ï¼š**
- `num_samples` (int): æµ‹è¯•æ ·æœ¬æ•°

**è¿”å›ï¼š**
- `dict`: æ€§èƒ½ç»Ÿè®¡

**ç¤ºä¾‹ï¼š**
```python
# åˆ›å»ºæ¨ç†å¼•æ“
engine = InferenceEngine(agent, device="cuda", batch_size=32)

# æ‰¹é‡æ¨ç†
observations = np.random.randn(32, 4)  # 32ä¸ªè§‚æµ‹
actions = engine.predict(observations)

# æ€§èƒ½æµ‹è¯•
stats = engine.benchmark(num_samples=10000)
print(f"å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_time']:.3f}ms")
```

## ğŸ”§ å·¥å…·ç±» API

### 1. Config ç±»

é…ç½®ç®¡ç†å·¥å…·ã€‚

#### åˆå§‹åŒ–

```python
Config(config=None)
```

**å‚æ•°ï¼š**
- `config` (dict/str, optional): é…ç½®å­—å…¸æˆ–æ–‡ä»¶è·¯å¾„

#### æ–¹æ³•

##### `get(key, default=None)`

è·å–é…ç½®å€¼ï¼Œæ”¯æŒç‚¹åˆ†å‰²è·¯å¾„ã€‚

**ç¤ºä¾‹ï¼š**
```python
config = Config()
lr = config.get("agent.lr", 3e-4)
hidden_dims = config.get("agent.hidden_dims", [64, 64])
```

##### `set(key, value)`

è®¾ç½®é…ç½®å€¼ã€‚

**ç¤ºä¾‹ï¼š**
```python
config.set("agent.lr", 1e-4)
config.set("training.max_episodes", 2000)
```

##### `load_from_file(filepath)` / `save_to_file(filepath)`

ä»æ–‡ä»¶åŠ è½½/ä¿å­˜é…ç½®ã€‚

**æ”¯æŒæ ¼å¼ï¼š** JSON, YAML

### 2. Logger ç±»

æ—¥å¿—è®°å½•å·¥å…·ã€‚

#### åˆå§‹åŒ–

```python
Logger(log_dir, use_tensorboard=True, use_wandb=False, wandb_config=None)
```

#### æ–¹æ³•

##### `log_scalar(tag, value, step)`

è®°å½•æ ‡é‡å€¼ã€‚

##### `log_histogram(tag, values, step)`

è®°å½•ç›´æ–¹å›¾ã€‚

##### `log_image(tag, image, step)`

è®°å½•å›¾åƒã€‚

**ç¤ºä¾‹ï¼š**
```python
logger = Logger("./logs/experiment_1")
logger.log_scalar("train/reward", reward, episode)
logger.log_histogram("train/actions", actions, episode)
```

## ğŸ“Š ç®—æ³•ç‰¹å®šé…ç½®

### PPO é…ç½®å‚æ•°

```python
ppo_config = {
    # ç½‘ç»œæ¶æ„
    "hidden_dims": [64, 64],
    
    # å­¦ä¹ å‚æ•°
    "lr": 3e-4,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    
    # PPOå‚æ•°
    "clip_ratio": 0.2,
    "ppo_epochs": 4,
    "mini_batch_size": 64,
    "value_loss_coef": 0.5,
    "entropy_coef": 0.01,
    "max_grad_norm": 0.5,
    
    # è®­ç»ƒå‚æ•°
    "update_interval": 2048,
    "normalize_advantages": True
}
```

### DQN é…ç½®å‚æ•°

```python
dqn_config = {
    # ç½‘ç»œæ¶æ„
    "hidden_dims": [128, 128],
    "dueling": True,
    
    # å­¦ä¹ å‚æ•°
    "lr": 1e-4,
    "gamma": 0.99,
    
    # DQNå‚æ•°
    "memory_size": 100000,
    "batch_size": 32,
    "target_update_freq": 1000,
    "double_dqn": True,
    
    # æ¢ç´¢å‚æ•°
    "epsilon_start": 1.0,
    "epsilon_end": 0.01,
    "epsilon_decay": 0.995,
    
    # è®­ç»ƒå‚æ•°
    "learning_starts": 1000,
    "train_freq": 4
}
```

### SAC é…ç½®å‚æ•°

```python
sac_config = {
    # ç½‘ç»œæ¶æ„
    "hidden_dims": [256, 256],
    
    # å­¦ä¹ å‚æ•°
    "actor_lr": 3e-4,
    "critic_lr": 3e-4,
    "alpha_lr": 3e-4,
    "gamma": 0.99,
    "tau": 0.005,
    
    # SACå‚æ•°
    "alpha": 0.2,
    "automatic_entropy_tuning": True,
    "target_entropy": None,
    
    # è®­ç»ƒå‚æ•°
    "memory_size": 1000000,
    "batch_size": 256,
    "learning_starts": 10000,
    "train_freq": 1,
    "gradient_steps": 1
}
```

### A3C é…ç½®å‚æ•°

```python
a3c_config = {
    # ç½‘ç»œæ¶æ„
    "hidden_dims": [128, 128],
    
    # å­¦ä¹ å‚æ•°
    "lr": 1e-4,
    "gamma": 0.99,
    "gae_lambda": 1.0,
    
    # A3Cå‚æ•°
    "num_workers": 8,
    "n_steps": 5,
    "value_loss_coef": 0.5,
    "entropy_coef": 0.01,
    "max_grad_norm": 40.0,
    
    # è®­ç»ƒå‚æ•°
    "update_interval": 5
}
```

## ğŸŒ ç¯å¢ƒé…ç½®é€‰é¡¹

### åŸºç¡€ç¯å¢ƒé…ç½®

```python
env_config = {
    # é¢„å¤„ç†
    "frame_stack": 1,           # å¸§å †å 
    "frame_skip": 1,            # å¸§è·³è·ƒ
    "resize": None,             # å›¾åƒç¼©æ”¾å°ºå¯¸ (height, width)
    "grayscale": False,         # è½¬æ¢ä¸ºç°åº¦å›¾
    
    # å½’ä¸€åŒ–
    "normalize_obs": False,     # è§‚æµ‹å½’ä¸€åŒ–
    "normalize_reward": False,  # å¥–åŠ±å½’ä¸€åŒ–
    "clip_obs": 10.0,          # è§‚æµ‹è£å‰ª
    "clip_reward": 1.0,        # å¥–åŠ±è£å‰ª
    
    # å›åˆæ§åˆ¶
    "max_episode_steps": None,  # æœ€å¤§æ­¥æ•°
    "noop_max": 30,            # åˆå§‹æ— æ“ä½œæ­¥æ•°
    
    # å‘é‡åŒ–ç¯å¢ƒ
    "num_envs": 1,             # å¹¶è¡Œç¯å¢ƒæ•°é‡
    "async_envs": False        # å¼‚æ­¥ç¯å¢ƒ
}
```

### è§†è§‰ç¯å¢ƒé…ç½®

```python
visual_env_config = {
    "frame_stack": 4,
    "frame_skip": 4,
    "resize": (84, 84),
    "grayscale": True,
    "normalize_obs": True,
    "clip_obs": 1.0
}
```

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—é…ç½®

### TensorBoard é…ç½®

```python
tensorboard_config = {
    "log_dir": "./logs",
    "use_tensorboard": True,
    "log_freq": 10,
    "histogram_freq": 100,
    "write_graph": True,
    "write_images": False
}
```

### Wandb é…ç½®

```python
wandb_config = {
    "use_wandb": True,
    "project": "agentrl-experiments",
    "name": "ppo-cartpole",
    "tags": ["ppo", "cartpole"],
    "notes": "PPO training on CartPole environment",
    "config": {
        "algorithm": "PPO",
        "environment": "CartPole-v1"
    }
}
```

## ğŸ” å›è°ƒç³»ç»Ÿ

### è‡ªå®šä¹‰å›è°ƒ

```python
class CustomCallback:
    def on_training_start(self, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
        pass
    
    def on_episode_end(self, episode, reward, **kwargs):
        """å›åˆç»“æŸæ—¶è°ƒç”¨"""
        if episode % 100 == 0:
            print(f"Episode {episode}, Reward: {reward}")
    
    def on_training_end(self, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨"""
        pass

# ä½¿ç”¨å›è°ƒ
trainer = PPOTrainer(agent, env)
trainer.add_callback(CustomCallback())
trainer.train()
```

### å†…ç½®å›è°ƒ

- `EarlyStoppingCallback`: æ—©åœå›è°ƒ
- `CheckpointCallback`: æ¨¡å‹ä¿å­˜å›è°ƒ
- `ProgressCallback`: è¿›åº¦æ˜¾ç¤ºå›è°ƒ
- `WandbCallback`: Wandbé›†æˆå›è°ƒ

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰ç½‘ç»œæ¶æ„

```python
from agentrl.core.network import MLP
import torch.nn as nn

class CustomNetwork(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.features = MLP(input_dim, [128, 128], 128)
        self.actor = nn.Linear(128, output_dim)
        self.critic = nn.Linear(128, 1)
    
    def forward(self, x):
        features = self.features(x)
        return self.actor(features), self.critic(features)

# ä½¿ç”¨è‡ªå®šä¹‰ç½‘ç»œ
agent_config = {
    "network_class": CustomNetwork,
    "network_kwargs": {"input_dim": 4, "output_dim": 2}
}
```

### 2. å¤šGPUè®­ç»ƒ

```python
# å¯ç”¨å¤šGPUæ”¯æŒ
config.set("training.use_gpu", True)
config.set("training.gpu_ids", [0, 1])
config.set("training.data_parallel", True)

trainer = PPOTrainer(agent, env, config)
trainer.train()
```

### 3. åˆ†å¸ƒå¼è®­ç»ƒ

```python
# åˆ†å¸ƒå¼é…ç½®
config.set("distributed.backend", "nccl")
config.set("distributed.init_method", "tcp://localhost:23456")
config.set("distributed.world_size", 4)
config.set("distributed.rank", 0)
```

## â— å¼‚å¸¸å¤„ç†

### å¸¸è§å¼‚å¸¸

- `ValueError`: å‚æ•°é…ç½®é”™è¯¯
- `RuntimeError`: è¿è¡Œæ—¶é”™è¯¯
- `NotImplementedError`: åŠŸèƒ½æœªå®ç°
- `ImportError`: ä¾èµ–åº“æœªå®‰è£…

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
config.set("debug.enabled", True)
config.set("debug.log_level", "DEBUG")
config.set("debug.save_intermediate", True)
```

è¿™ä¸ªAPIæ–‡æ¡£æä¾›äº†AgentRLæ¡†æ¶çš„å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼Œæ¶µç›–äº†æ‰€æœ‰ä¸»è¦åŠŸèƒ½å’Œé…ç½®é€‰é¡¹ã€‚é€šè¿‡è¿™äº›æ¥å£ï¼Œç”¨æˆ·å¯ä»¥è½»æ¾åœ°è®­ç»ƒã€è¯„ä¼°å’Œéƒ¨ç½²å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚