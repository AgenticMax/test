# AgentRL - é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ¡†æ¶

ä¸€ä¸ªä¸“ä¸ºæå‡æ¨ç†å’Œè®­ç»ƒæ•ˆç‡è€Œè®¾è®¡çš„ç°ä»£åŒ–å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ¡†æ¶ã€‚

## ğŸš€ ç‰¹æ€§

- **é«˜æ•ˆè®­ç»ƒ**: æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒï¼ŒGPUåŠ é€Ÿï¼Œå†…å­˜ä¼˜åŒ–
- **çµæ´»æ¶æ„**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¤šç§RLç®—æ³•ï¼ˆDQN, PPO, SAC, A3Cç­‰ï¼‰
- **æ™ºèƒ½æ¨ç†**: ä¼˜åŒ–çš„æ¨ç†å¼•æ“ï¼Œæ”¯æŒæ‰¹é‡æ¨ç†å’Œæ¨¡å‹å‹ç¼©
- **å®æ—¶ç›‘æ§**: é›†æˆTensorBoardå’ŒWandbï¼Œå®æ—¶è®­ç»ƒç›‘æ§
- **æ˜“äºæ‰©å±•**: æ’ä»¶åŒ–ç®—æ³•æ¥å£ï¼Œä¾¿äºæ·»åŠ æ–°ç®—æ³•
- **åˆ†å¸ƒå¼æ”¯æŒ**: æ”¯æŒå¤šæœºåˆ†å¸ƒå¼è®­ç»ƒ

## ğŸ“¦ å®‰è£…

```bash
pip install -r requirements.txt
```

## ğŸ¯ å¿«é€Ÿå¼€å§‹

```python
from agentrl import Agent, Environment, PPOTrainer

# åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
env = Environment("CartPole-v1")
agent = Agent("PPO", env.observation_space, env.action_space)

# å¼€å§‹è®­ç»ƒ
trainer = PPOTrainer(agent, env)
trainer.train(episodes=1000)

# è¯„ä¼°æ™ºèƒ½ä½“
rewards = trainer.evaluate(episodes=100)
print(f"Average reward: {rewards.mean()}")
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
agentrl/
â”œâ”€â”€ core/           # æ ¸å¿ƒç»„ä»¶
â”œâ”€â”€ algorithms/     # RLç®—æ³•å®ç°
â”œâ”€â”€ environments/   # ç¯å¢ƒæ¥å£
â”œâ”€â”€ utils/          # å·¥å…·å‡½æ•°
â”œâ”€â”€ training/       # è®­ç»ƒç›¸å…³
â”œâ”€â”€ inference/      # æ¨ç†å¼•æ“
â””â”€â”€ examples/       # ç¤ºä¾‹ä»£ç 
```

## ğŸ”§ æ”¯æŒçš„ç®—æ³•

- **Deep Q-Network (DQN)**: ç¦»æ•£åŠ¨ä½œç©ºé—´çš„å€¼å‡½æ•°æ–¹æ³•
- **Proximal Policy Optimization (PPO)**: ç¨³å®šçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- **Soft Actor-Critic (SAC)**: è¿ç»­æ§åˆ¶çš„æœ€å¤§ç†µæ–¹æ³•
- **Asynchronous Advantage Actor-Critic (A3C)**: å¼‚æ­¥å¹¶è¡Œè®­ç»ƒ
- **Twin Delayed Deep Deterministic (TD3)**: æ”¹è¿›çš„ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

- å‘é‡åŒ–ç¯å¢ƒå¤„ç†
- ç»éªŒå›æ”¾ç¼“å†²åŒºä¼˜åŒ–
- åŠ¨æ€æ‰¹é‡å¤§å°è°ƒæ•´
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ¨¡å‹é‡åŒ–å’Œå‰ªæ

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License